{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local setup: install deps with `pip install -r requirements.txt`, download Kaggle data to `../data/`, then run cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "D9qk2UUhkrpH",
    "outputId": "6f001f2d-e488-49b6-9772-d96a0e2d6d8b"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import torch, pandas as pd\n",
    "print(\"RDKit version:\", Chem.rdBase.rdkitVersion)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKl1I6ySmsPU",
    "outputId": "dbb1d2c1-aec4-4564-8dd1-8c549e7c2e60"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "train_df = pd.read_csv(DATA_DIR / 'rxn_train.csv')\n",
    "print(train_df.columns)\n",
    "print(train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNNCG94umxoX",
    "outputId": "c625340a-64c9-47ac-b55b-a9c28b847610"
   },
   "outputs": [],
   "source": [
    "example_rxn = train_df.iloc[0, 2] # first row's reaction SMILES\n",
    "print(\"Example reaction SMILES:\", example_rxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrBi94MWnE91",
    "outputId": "6b09e059-8bb4-4c37-aec3-5d27621c0165"
   },
   "outputs": [],
   "source": [
    "sample_df = train_df.sample(100, random_state=42)\n",
    "sample_smiles = sample_df.iloc[:, 2] # the reaction SMILES column\n",
    "print(sample_smiles.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qQmjB3hsnLXz",
    "outputId": "e62c7a26-6d66-4595-9e5f-ab3e05a33fc6"
   },
   "outputs": [],
   "source": [
    "rxn_str = sample_smiles.iloc[0]\n",
    "reactants_str, product_str = rxn_str.split(\">>\")\n",
    "print(\"Reactants:\", reactants_str)\n",
    "print(\"Product:\", product_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhOZ_-3xnOLL",
    "outputId": "5cdbbfb8-bff7-4234-ad71-d1954321ab45"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# function to remove mapping via RDKit canonicalization\n",
    "def canonicalize_smiles(smi):\n",
    "  mol = Chem.MolFromSmiles(smi)\n",
    "  if mol:\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "  return None\n",
    "print(\"Before:\", reactants_str)\n",
    "print(\"After:\", canonicalize_smiles(reactants_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Br0PcBDNndH7"
   },
   "outputs": [],
   "source": [
    "assert sample_smiles.str.contains(\">>\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SurSKnhnuAb",
    "outputId": "eaf7a24b-eee8-4ffb-c38e-2f0f2925477d"
   },
   "outputs": [],
   "source": [
    "test_smi = \"CCO\" # ethanol, a simple SMILES\n",
    "mol = Chem.MolFromSmiles(test_smi)\n",
    "print(\"RDKit Mol object:\", mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxnOC_i7nvx-",
    "outputId": "008ef41d-1a8c-423f-fbb0-85ab94bca034"
   },
   "outputs": [],
   "source": [
    "canonical = Chem.MolToSmiles(mol)\n",
    "print(\"Canonical SMILES from Mol:\", canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v16XpOsnxYA",
    "outputId": "311c514b-2dda-4fc1-a238-a5a8ebadabc0"
   },
   "outputs": [],
   "source": [
    "react_mol = Chem.MolFromSmiles(reactants_str)\n",
    "prod_mol = Chem.MolFromSmiles(product_str)\n",
    "print(\"Reactant Mol:\", react_mol, \"Product Mol:\", prod_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WA21tm-_n0yh",
    "outputId": "d251247f-d0bf-4083-c1e0-b1e856429fb9"
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "mw = Descriptors.MolWt(prod_mol) # molecular weight of product\n",
    "print(\"Product molecular weight:\", mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5reT06Uv-s1",
    "outputId": "2d8094fc-1c8e-4fae-dee9-ac0f9143264e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem # RDKit for chemical informatics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Use GPU if available for faster training\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSmvtXTNxAAG",
    "outputId": "be8da9b5-9fc3-4f38-d292-5ba403fbae4f"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "train_file = DATA_DIR / 'rxn_train.csv'\n",
    "val_file = DATA_DIR / 'rxn_val.csv'\n",
    "test_file = DATA_DIR / 'rxn_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "val_df = pd.read_csv(val_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "# The dataset might have columns like ['ID', 'Class', 'reactants>>product'].\n",
    "# We'll assume the reaction SMILES is the last column (index 2 if zero-indexed).\n",
    "train_rxns = train_df.iloc[:, -1].values\n",
    "val_rxns = val_df.iloc[:, -1].values\n",
    "test_rxns = test_df.iloc[:, -1].values\n",
    "print('Number of training reactions:', len(train_rxns))\n",
    "print('Example reaction (raw):', train_rxns[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-SQPXFlxTHx",
    "outputId": "3e90ea2d-fd04-4c8e-f64f-a68ec13d4df7"
   },
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles):\n",
    "  try:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "      return None\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "train_reactants, train_products = [], []\n",
    "for rxn in train_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  # Canonicalize each molecule on both sides\n",
    "  # Reactants might have multiple molecules separated by '.', handle each\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  # Skip this reaction if any part failed to canonicalize\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  # Rejoin multi-molecule SMILES with '.' (order is preserved)\n",
    "  train_reactants.append('.'.join(react_parts))\n",
    "  train_products.append('.'.join(prod_parts))\n",
    "\n",
    "val_reactants, val_products = [], []\n",
    "for rxn in val_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  val_reactants.append('.'.join(react_parts))\n",
    "  val_products.append('.'.join(prod_parts))\n",
    "test_reactants, test_products = [], []\n",
    "for rxn in test_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  test_reactants.append('.'.join(react_parts))\n",
    "  test_products.append('.'.join(prod_parts))\n",
    "\n",
    "\n",
    "print(f\"After canonicalization: {len(train_reactants)} training reactions, {len(val_reactants)} validation, {len(test_reactants)} test.\")\n",
    "# Print a sample to see the effect of canonicalization\n",
    "print(\"Sample reactants (canonical):\", train_reactants[0])\n",
    "print(\"Sample product (canonical):\", train_products[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlmfU4b3yVlF",
    "outputId": "3591fd41-9076-4a10-9ae8-8bf62bac5d9d"
   },
   "outputs": [],
   "source": [
    "# Build character vocabulary from the training set\n",
    "chars = set()\n",
    "for smi in train_reactants + train_products:\n",
    "  chars.update(list(smi))\n",
    "# Also ensure all characters in val and test are covered (to avoid unknown tokens)\n",
    "for smi in val_reactants + val_products + test_reactants + test_products:\n",
    "  chars.update(list(smi))\n",
    "# Define special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "# Create vocab list: special tokens + sorted characters (for consistency)\n",
    "char_list = sorted(list(chars))\n",
    "vocab = special_tokens + char_list\n",
    "# Create mappings from token to index and index to token\n",
    "token_to_idx = {tok: i for i, tok in enumerate(vocab)}\n",
    "idx_to_token = {i: tok for i, tok in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Tokens:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cvm7tP34yh6I",
    "outputId": "40d2bc8b-6844-4171-b7d7-be132507f028"
   },
   "outputs": [],
   "source": [
    "# Utility function to encode a SMILES string to a list of token indices\n",
    "def encode_smiles(smi, token_to_idx):\n",
    "  return [token_to_idx[ch] for ch in smi]\n",
    "\n",
    "# Sample a smaller subset of the dataframes for debugging\n",
    "sample_size = 200  # reduced for stability on Mac\n",
    "sample_size = min(sample_size, len(train_df))\n",
    "val_size = max(1, int(sample_size * 0.2))\n",
    "train_df_sampled = train_df.sample(n=sample_size, random_state=42)\n",
    "val_df_sampled = val_df.sample(n=min(val_size, len(val_df)), random_state=42)  # smaller validation set\n",
    "test_df_sampled = test_df.sample(n=min(val_size, len(test_df)), random_state=42)  # smaller test set\n",
    "\n",
    "train_rxns = train_df_sampled.iloc[:, -1].values  # numpy array of reaction strings\n",
    "val_rxns = val_df_sampled.iloc[:, -1].values\n",
    "test_rxns = test_df_sampled.iloc[:, -1].values\n",
    "\n",
    "print(\"Number of training reactions (sampled):\", len(train_rxns))\n",
    "print(\"Example reaction (raw):\", train_rxns[0])\n",
    "\n",
    "\n",
    "train_reactants, train_products = [], []\n",
    "for rxn in train_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  # Canonicalize each molecule on both sides\n",
    "  # Reactants might have multiple molecules separated by '.', handle each\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  # Skip this reaction if any part failed to canonicalize\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  # Rejoin multi-molecule SMILES with '.' (order is preserved)\n",
    "  train_reactants.append('.'.join(react_parts))\n",
    "  train_products.append('.'.join(prod_parts))\n",
    "\n",
    "val_reactants, val_products = [], []\n",
    "for rxn in val_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  val_reactants.append('.'.join(react_parts))\n",
    "  val_products.append('.'.join(prod_parts))\n",
    "test_reactants, test_products = [], []\n",
    "for rxn in test_rxns:\n",
    "  react_str, prod_str = rxn.split(\">>\")\n",
    "  react_parts = [canonicalize_smiles(r) for r in react_str.split('.')]\n",
    "  prod_parts = [canonicalize_smiles(p) for p in prod_str.split('.')]\n",
    "  if None in react_parts or None in prod_parts:\n",
    "    continue\n",
    "  test_reactants.append('.'.join(react_parts))\n",
    "  test_products.append('.'.join(prod_parts))\n",
    "\n",
    "\n",
    "print(f\"After canonicalization: {len(train_reactants)} training reactions, {len(val_reactants)} validation, {len(test_reactants)} test.\")\n",
    "# Print a sample to see the effect of canonicalization\n",
    "print(\"Sample reactants (canonical):\", train_reactants[0])\n",
    "print(\"Sample product (canonical):\", train_products[0])\n",
    "\n",
    "\n",
    "# Build character vocabulary from the training set\n",
    "chars = set()\n",
    "for smi in train_reactants + train_products:\n",
    "  chars.update(list(smi))\n",
    "# Also ensure all characters in val and test are covered (to avoid unknown tokens)\n",
    "for smi in val_reactants + val_products + test_reactants + test_products:\n",
    "  chars.update(list(smi))\n",
    "# Define special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "# Create vocab list: special tokens + sorted characters (for consistency)\n",
    "# Sorting the characters is not strictly necessary, but it ensures a stable order\n",
    "char_list = sorted(list(chars))\n",
    "vocab = special_tokens + char_list\n",
    "# Create mappings from token to index and index to token\n",
    "token_to_idx = {tok: i for i, tok in enumerate(vocab)}\n",
    "idx_to_token = {i: tok for i, tok in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "# print(\"Tokens:\", vocab) # Optional: print full vocab\n",
    "\n",
    "# Encode all reactants and products\n",
    "train_enc_reactants = [encode_smiles(s, token_to_idx) for s in train_reactants]\n",
    "train_enc_products = [encode_smiles(s, token_to_idx) +\n",
    "[token_to_idx[EOS_TOKEN]] for s in train_products]\n",
    "val_enc_reactants = [encode_smiles(s, token_to_idx) for s in val_reactants]\n",
    "val_enc_products = [encode_smiles(s, token_to_idx) + [token_to_idx[EOS_TOKEN]]\n",
    "for s in val_products]\n",
    "test_enc_reactants = [encode_smiles(s, token_to_idx) for s in test_reactants]\n",
    "test_enc_products = [encode_smiles(s, token_to_idx) + [token_to_idx[EOS_TOKEN]]\n",
    "for s in test_products]\n",
    "\n",
    "# Determine max lengths for padding\n",
    "max_len_react = max(len(seq) for seq in train_enc_reactants + val_enc_reactants + test_enc_reactants)\n",
    "max_len_prod = max(len(seq) for seq in train_enc_products + val_enc_products + test_enc_products)\n",
    "# Cap sequence lengths to avoid overly long sequences that can destabilize training on CPU/MPS\n",
    "MAX_LEN_CAP = 256\n",
    "max_len_react = min(max_len_react, MAX_LEN_CAP)\n",
    "max_len_prod = min(max_len_prod, MAX_LEN_CAP)\n",
    "print(\"Max reactant length (capped):\", max_len_react)\n",
    "print(\"Max product length (including <eos>) (capped):\", max_len_prod)\n",
    "\n",
    "# Pad sequences to the max length\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "  seq = seq[:max_len]\n",
    "  return seq + [pad_idx] * (max_len - len(seq))\n",
    "\n",
    "train_enc_reactants = [pad_sequence(seq, max_len_react, token_to_idx[PAD_TOKEN]) for seq in train_enc_reactants]\n",
    "train_enc_products = [pad_sequence(seq, max_len_prod, token_to_idx[PAD_TOKEN]) for seq in train_enc_products]\n",
    "val_enc_reactants = [pad_sequence(seq, max_len_react, token_to_idx[PAD_TOKEN]) for seq in val_enc_reactants]\n",
    "val_enc_products = [pad_sequence(seq, max_len_prod, token_to_idx[PAD_TOKEN]) for seq in val_enc_products]\n",
    "test_enc_reactants = [pad_sequence(seq, max_len_react, token_to_idx[PAD_TOKEN]) for seq in test_enc_reactants]\n",
    "test_enc_products = [pad_sequence(seq, max_len_prod, token_to_idx[PAD_TOKEN]) for seq in test_enc_products]\n",
    "# Convert lists to numpy arrays, then to torch tensors\n",
    "train_enc_reactants = torch.tensor(train_enc_reactants, dtype=torch.long)\n",
    "train_enc_products = torch.tensor(train_enc_products, dtype=torch.long)\n",
    "val_enc_reactants = torch.tensor(val_enc_reactants, dtype=torch.long)\n",
    "val_enc_products = torch.tensor(val_enc_products, dtype=torch.long)\n",
    "test_enc_reactants = torch.tensor(test_enc_reactants, dtype=torch.long)\n",
    "test_enc_products = torch.tensor(test_enc_products, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 64 # you can adjust this\n",
    "train_dataset = TensorDataset(train_enc_reactants, train_enc_products)\n",
    "val_dataset = TensorDataset(val_enc_reactants, val_enc_products)\n",
    "test_dataset = TensorDataset(test_enc_reactants, test_enc_products)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#batch_size=1 for easier evaluation\n",
    "print(\"Data preparation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoaEqXaPeozg",
    "outputId": "abe83495-2ea1-41cf-aa49-0b55b202c29e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(train_enc_reactants, train_enc_products)\n",
    "val_dataset = TensorDataset(val_enc_reactants, val_enc_products)\n",
    "test_dataset = TensorDataset(test_enc_reactants, test_enc_products)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=token_to_idx[PAD_TOKEN])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, hidden = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=token_to_idx[PAD_TOKEN])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        embedded = self.embedding(input_token)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output_logits = self.fc_out(output)\n",
    "        return output_logits, hidden\n",
    "\n",
    "encoder = Encoder(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
    "decoder = Decoder(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
    "\n",
    "xent = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN], reduction='none')\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=5e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    encoder.train(); decoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (reactant_batch, product_batch) in enumerate(train_loader):\n",
    "        reactant_batch = reactant_batch.to(device)\n",
    "        product_batch = product_batch.to(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(reactant_batch)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        batch_size_curr = reactant_batch.size(0)\n",
    "        decoder_input = torch.full((batch_size_curr, 1), token_to_idx[SOS_TOKEN], dtype=torch.long, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_accum = 0.0\n",
    "        token_count = 0\n",
    "\n",
    "        for t in range(max_len_prod):\n",
    "            output_logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            output_logits = output_logits.squeeze(1)\n",
    "            if not torch.isfinite(output_logits).all():\n",
    "                print(f\"Skipping batch {batch_idx} due to non-finite logits at step {t}\")\n",
    "                token_count = 0\n",
    "                break\n",
    "            target_token = product_batch[:, t]\n",
    "            mask = target_token != token_to_idx[PAD_TOKEN]\n",
    "            if mask.any():\n",
    "                per_token = xent(output_logits, target_token)\n",
    "                loss_accum += (per_token * mask).sum()\n",
    "                token_count += mask.sum().item()\n",
    "            decoder_input = target_token.unsqueeze(1)\n",
    "\n",
    "        if token_count == 0:\n",
    "            print(f\"Skipping batch {batch_idx} (no valid targets)\")\n",
    "            continue\n",
    "\n",
    "        loss = loss_accum / token_count\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"Skipping batch {batch_idx} due to invalid loss value\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if len(train_loader) == 0:\n",
    "        raise RuntimeError(\"Training loader is empty after preprocessing\")\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    encoder.eval(); decoder.eval()\n",
    "    val_loss = 0.0\n",
    "    val_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for reactant_batch, product_batch in val_loader:\n",
    "            reactant_batch = reactant_batch.to(device)\n",
    "            product_batch = product_batch.to(device)\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(reactant_batch)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = torch.full((reactant_batch.size(0), 1), token_to_idx[SOS_TOKEN], dtype=torch.long, device=device)\n",
    "\n",
    "            for t in range(max_len_prod):\n",
    "                output_logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                output_logits = output_logits.squeeze(1)\n",
    "                if not torch.isfinite(output_logits).all():\n",
    "                    continue\n",
    "                target_token = product_batch[:, t]\n",
    "                mask = target_token != token_to_idx[PAD_TOKEN]\n",
    "                if mask.any():\n",
    "                    per_token = xent(output_logits, target_token)\n",
    "                    val_loss += (per_token * mask).sum().item()\n",
    "                    val_tokens += mask.sum().item()\n",
    "                decoder_input = target_token.unsqueeze(1)\n",
    "\n",
    "    if len(val_loader) == 0 or val_tokens == 0:\n",
    "        raise RuntimeError(\"Validation loader produced no valid targets\")\n",
    "    val_loss = val_loss / val_tokens\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWeIJBYSMxMK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "# -------- helper: quick token utilities ----------\n",
    "SPECIAL_IDS = set([\n",
    "    # fill at runtime (do this after token_to_idx exists):\n",
    "    # token_to_idx[PAD_TOKEN], token_to_idx[SOS_TOKEN], token_to_idx[EOS_TOKEN]\n",
    "])\n",
    "\n",
    "def ids_to_smiles(id_list, idx_to_token):\n",
    "    return \"\".join(\n",
    "        idx_to_token[i] for i in id_list\n",
    "        if i not in SPECIAL_IDS\n",
    "    )\n",
    "\n",
    "def is_balanced_brackets(s):\n",
    "    # quick sanity check for () and [] balance while decoding\n",
    "    stack = []\n",
    "    pairs = {')': '(', ']': '['}\n",
    "    for ch in s:\n",
    "        if ch in '([':\n",
    "            stack.append(ch)\n",
    "        elif ch in ')]':\n",
    "            if not stack or stack[-1] != pairs[ch]:\n",
    "                return False\n",
    "            stack.pop()\n",
    "    # allow being \"open\" during partial decoding; just disallow over-closing\n",
    "    return True\n",
    "\n",
    "def partial_valid(s):\n",
    "    # very lightweight gate: allow partial strings that still *could* form SMILES.\n",
    "    # We don\u2019t require RDKit to parse partials (it\u2019s too strict mid-string).\n",
    "    # 1) Don\u2019t allow closing brackets/parentheses past balance\n",
    "    # 2) Don\u2019t allow two dots in a row, etc. (minimal rules)\n",
    "    if not is_balanced_brackets(s):\n",
    "        return False\n",
    "    if '..' in s:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau-style attention: score(h_t, h_s) = v^T tanh(W1*h_s + W2*h_t)\n",
    "    Shapes:\n",
    "      encoder_outputs: (B, S, H_e)\n",
    "      decoder_hidden_t: (B, H_d)\n",
    "      returns: (B, S) attention weights\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hidden, dec_hidden):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(enc_hidden, dec_hidden, bias=False)\n",
    "        self.W2 = nn.Linear(dec_hidden, dec_hidden, bias=False)\n",
    "        self.v  = nn.Linear(dec_hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, dec_hidden_t):\n",
    "        # encoder_proj: (B, S, H_d)\n",
    "        encoder_proj = self.W1(encoder_outputs)\n",
    "        # dec_proj: (B, 1, H_d)\n",
    "        dec_proj = self.W2(dec_hidden_t).unsqueeze(1)\n",
    "        # energies: (B, S, 1) -> (B, S)\n",
    "        energies = self.v(torch.tanh(encoder_proj + dec_proj)).squeeze(-1)\n",
    "        attn_weights = F.softmax(energies, dim=1)\n",
    "        return attn_weights  # (B, S)\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with attention. Projects encoder_outputs to decoder hidden size if needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, dec_hidden, num_layers=1, pad_idx=0, enc_hidden=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_size, dec_hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(dec_hidden + (enc_hidden if enc_hidden else dec_hidden), vocab_size)\n",
    "\n",
    "        self.enc_to_dec = None\n",
    "        self.attn = None\n",
    "        if enc_hidden is None or enc_hidden == dec_hidden:\n",
    "            # encoder and decoder hidden sizes match\n",
    "            self.attn = AdditiveAttention(dec_hidden, dec_hidden)\n",
    "            self.enc_to_dec = nn.Identity()\n",
    "            self.enc_hidden_dim = dec_hidden\n",
    "        else:\n",
    "            # project encoder outputs to decoder hidden size\n",
    "            self.enc_to_dec = nn.Linear(enc_hidden, dec_hidden, bias=False)\n",
    "            self.attn = AdditiveAttention(dec_hidden, dec_hidden)\n",
    "            self.enc_hidden_dim = dec_hidden\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input_token: (B, 1)\n",
    "        hidden: (h, c) with shapes (num_layers, B, dec_hidden)\n",
    "        encoder_outputs: (B, S, enc_hidden)\n",
    "        \"\"\"\n",
    "        B, S, _ = encoder_outputs.size()\n",
    "        # project encoder outputs if needed\n",
    "        enc_proj = self.enc_to_dec(encoder_outputs)  # (B, S, dec_hidden)\n",
    "\n",
    "        emb = self.embedding(input_token)            # (B, 1, E)\n",
    "        out, hidden = self.lstm(emb, hidden)         # out: (B, 1, dec_hidden)\n",
    "        dec_h_t = hidden[0][-1]                      # last layer (B, dec_hidden)\n",
    "\n",
    "        # attention over encoder states (using projected enc states)\n",
    "        attn_w = self.attn(enc_proj, dec_h_t)        # (B, S)\n",
    "        context = torch.bmm(attn_w.unsqueeze(1), enc_proj)  # (B, 1, dec_hidden)\n",
    "\n",
    "        # concat context with decoder output (repeat out to shape (B, 1, dec_hidden))\n",
    "        combined = torch.cat([out, context], dim=-1)  # (B, 1, dec_hidden + dec_hidden)\n",
    "        logits = self.fc_out(combined)               # (B, 1, vocab)\n",
    "        return logits, hidden, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEXqydqRNHg4"
   },
   "outputs": [],
   "source": [
    "SPECIAL_IDS = set([token_to_idx[PAD_TOKEN], token_to_idx[SOS_TOKEN], token_to_idx[EOS_TOKEN]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwtJLgO0zYPu",
    "outputId": "1ba4a0d1-d67f-4224-c825-feb991231791"
   },
   "outputs": [],
   "source": [
    "# Function to perform greedy decoding for one reactant sequence\n",
    "def predict_greedy(reactant_seq):\n",
    "    encoder_outputs, encoder_hidden = encoder(reactant_seq)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.full((1, 1), token_to_idx[SOS_TOKEN], dtype=torch.long, device=device)  # start with <sos>\n",
    "\n",
    "    predicted_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(max_len_prod):\n",
    "            output_logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            output_logits = output_logits.squeeze(1)  # shape (1, vocab_size)\n",
    "\n",
    "            # Pick the token with highest probability\n",
    "            top_token = output_logits.argmax(dim=1)  # tensor of shape (1,)\n",
    "            top_idx = top_token.item()\n",
    "\n",
    "            if top_idx == token_to_idx[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "            predicted_tokens.append(top_idx)\n",
    "            decoder_input = top_token.unsqueeze(1)  # input next token\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Evaluation loop\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "top1_correct = 0\n",
    "total = 0\n",
    "valid_predictions = 0\n",
    "\n",
    "for reactant_seq, true_product_seq in test_loader:\n",
    "    # Ensure reactant_seq is 2D (batch_size, seq_len) for predict_greedy\n",
    "    reactant_seq = reactant_seq.to(device)\n",
    "    true_product_seq = true_product_seq.to(device)\n",
    "\n",
    "    # Since predict_greedy takes one sequence at a time, iterate through the batch\n",
    "    for i in range(reactant_seq.size(0)):\n",
    "        single_reactant_seq = reactant_seq[i].unsqueeze(0) # Shape (1, seq_len) for single prediction\n",
    "\n",
    "        # Remove PAD and EOS from target sequence\n",
    "        true_tokens = true_product_seq[i].squeeze().tolist()\n",
    "        if token_to_idx[PAD_TOKEN] in true_tokens:\n",
    "            true_tokens = true_tokens[:true_tokens.index(token_to_idx[PAD_TOKEN])]\n",
    "        if true_tokens and true_tokens[-1] == token_to_idx[EOS_TOKEN]:\n",
    "            true_tokens = true_tokens[:-1]\n",
    "\n",
    "        # Correctly map token indices to characters\n",
    "        true_smiles = \"\".join([idx_to_token[t] for t in true_tokens])\n",
    "        true_canon = canonicalize_smiles(true_smiles)\n",
    "\n",
    "        # Predict with greedy decoding\n",
    "        predicted_token_seq = predict_greedy(single_reactant_seq)\n",
    "\n",
    "        # Convert token sequence to SMILES string\n",
    "        raw_pred_tokens = [idx_to_token[t] for t in predicted_token_seq]\n",
    "        pred_smiles = \"\".join(raw_pred_tokens).replace(\" \", \"\").replace(\"][\", \"].[\")\n",
    "\n",
    "        # Sanitize brackets: ensure balanced []\n",
    "        if pred_smiles.count('[') != pred_smiles.count(']'):\n",
    "            pred_smiles = \"\"\n",
    "\n",
    "        # Canonicalize predicted SMILES\n",
    "        pred_canon = None\n",
    "        if pred_smiles != \"\":\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(pred_smiles)\n",
    "                if mol:\n",
    "                    pred_canon = Chem.MolToSmiles(mol, canonical=True)\n",
    "                    valid_predictions += 1\n",
    "            except:\n",
    "                pred_canon = None\n",
    "\n",
    "        # Check match with true product\n",
    "        if pred_canon is not None and true_canon is not None and pred_canon == true_canon:\n",
    "            top1_correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "# Final stats\n",
    "top1_acc = top1_correct / total * 100\n",
    "valid_percent = valid_predictions / total * 100\n",
    "\n",
    "print(f\"Baseline Seq2Seq Greedy Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "print(f\"Valid product predictions: {valid_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V_EndGYnOyR"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_attention_with_constraints(\n",
    "    encoder_attn, decoder_attn, input_seq,\n",
    "    sos_idx, eos_idx, max_len,\n",
    "    idx_to_token,\n",
    "    top_n=10\n",
    "):\n",
    "    encoder_attn.eval(); decoder_attn.eval()\n",
    "    enc_outputs, enc_hidden = encoder_attn(input_seq)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input  = torch.tensor([[sos_idx]], device=device)\n",
    "\n",
    "    pred_ids = []\n",
    "    partial = \"\"\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, dec_hidden, _ = decoder_attn(dec_input, dec_hidden, enc_outputs)\n",
    "        logits = logits.squeeze(1)                 # (1, V)\n",
    "        probs  = F.log_softmax(logits, dim=-1)[0]  # (V,)\n",
    "\n",
    "        next_idx = int(torch.argmax(probs).item())\n",
    "        top_idx  = torch.topk(probs, k=min(top_n, probs.numel())).indices.tolist()\n",
    "\n",
    "        chosen = None\n",
    "        for cand in top_idx:\n",
    "            if cand == eos_idx:\n",
    "                chosen = cand\n",
    "                break\n",
    "            trial = partial + (idx_to_token[cand] if cand not in SPECIAL_IDS else \"\")\n",
    "            if partial_valid(trial):\n",
    "                chosen = cand\n",
    "                break\n",
    "\n",
    "        if chosen is None:\n",
    "            chosen = next_idx\n",
    "\n",
    "        if chosen == eos_idx:\n",
    "            break\n",
    "\n",
    "        pred_ids.append(chosen)\n",
    "        if chosen not in SPECIAL_IDS:\n",
    "            partial += idx_to_token[chosen]\n",
    "\n",
    "        dec_input = torch.tensor([[chosen]], device=device)\n",
    "\n",
    "    return pred_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np0QzSkjWRbi",
    "outputId": "e7827fe4-e0d2-414d-e64d-379597fb308f"
   },
   "outputs": [],
   "source": [
    "# Print a few example predictions vs true\n",
    "for i in range(3):\n",
    "    reactant = test_enc_reactants[i].unsqueeze(0).to(device)  # take i-th test example\n",
    "    true_prod = test_products[i]\n",
    "\n",
    "    pred_tokens = predict_greedy(reactant)\n",
    "    pred_smiles = \"\".join([idx_to_token[t] for t in pred_tokens])\n",
    "\n",
    "    print(f\"Reactants: {test_reactants[i]}\")\n",
    "    print(f\"True Product: {true_prod}\")\n",
    "    print(f\"Predicted Product: {pred_smiles}\")\n",
    "\n",
    "    mol = Chem.MolFromSmiles(pred_smiles)\n",
    "    print(f\"Valid SMILES: {mol is not None}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# Decoder with Attention\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, embed_size, padding_idx=token_to_idx[PAD_TOKEN]\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # Simple dot-product attention: no learnable parameter for scores\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        # input_token: (batch, 1)\n",
    "        # hidden: tuple (h, c)\n",
    "        # encoder_outputs: (batch, seq_len, hidden_size)\n",
    "\n",
    "        embedded = self.embedding(input_token)  # (batch, 1, embed_size)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch, 1, hidden_size)\n",
    "\n",
    "        # Attention scores: dot product between encoder_outputs and lstm_out\n",
    "        attn_scores = torch.bmm(encoder_outputs, lstm_out.transpose(1, 2))\n",
    "        # (batch, seq_len, 1)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        # (batch, seq_len, 1), normalized weights\n",
    "\n",
    "        # Context vector: weighted sum of encoder_outputs\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs)\n",
    "        # (batch, 1, hidden_size)\n",
    "\n",
    "        # Concatenate LSTM output and context\n",
    "        combined = torch.cat([lstm_out, context], dim=2)\n",
    "        # (batch, 1, hidden_size*2)\n",
    "\n",
    "        # Mix them with a linear layer + tanh\n",
    "        combined = torch.tanh(self.attn_combine(combined))  # (batch, 1, hidden_size)\n",
    "\n",
    "        # Final output logits\n",
    "        output_logits = self.fc_out(combined)  # (batch, 1, output_size)\n",
    "\n",
    "        return output_logits, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubAfewgrWdYe",
    "outputId": "37df5d87-596c-4b6a-fe8a-be85e02b25f5"
   },
   "outputs": [],
   "source": [
    "# Initialize new model with attention\n",
    "encoder_attn = Encoder(\n",
    "    input_size=vocab_size,\n",
    "    embed_size=EMBEDDING_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "decoder_attn = DecoderWithAttention(\n",
    "    output_size=vocab_size,\n",
    "    embed_size=EMBEDDING_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "criterion_attn = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN], reduction='none')\n",
    "optimizer_attn = optim.Adam(\n",
    "    list(encoder_attn.parameters()) + list(decoder_attn.parameters()),\n",
    "    lr=5e-4\n",
    ")\n",
    "\n",
    "num_epochs_attn = 5\n",
    "for epoch in range(1, num_epochs_attn + 1):\n",
    "    encoder_attn.train(); decoder_attn.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (reactant_batch, product_batch) in enumerate(train_loader):\n",
    "        reactant_batch = reactant_batch.to(device)\n",
    "        product_batch = product_batch.to(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder_attn(reactant_batch)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.full(\n",
    "            (reactant_batch.size(0), 1),\n",
    "            token_to_idx[SOS_TOKEN],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        optimizer_attn.zero_grad()\n",
    "        loss_accum = 0.0\n",
    "        token_count = 0\n",
    "\n",
    "        for t in range(max_len_prod):\n",
    "            output_logits, decoder_hidden, attn_weights = decoder_attn(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            output_logits = output_logits.squeeze(1)  # (batch, vocab)\n",
    "            if not torch.isfinite(output_logits).all():\n",
    "                print(f\"[Attn] Skipping batch {batch_idx} due to non-finite logits at step {t}\")\n",
    "                token_count = 0\n",
    "                break\n",
    "            target_token = product_batch[:, t]\n",
    "            mask = target_token != token_to_idx[PAD_TOKEN]\n",
    "            if mask.any():\n",
    "                per_token = criterion_attn(output_logits, target_token)\n",
    "                loss_accum += (per_token * mask).sum()\n",
    "                token_count += mask.sum().item()\n",
    "\n",
    "            # Teacher forcing\n",
    "            decoder_input = target_token.unsqueeze(1)\n",
    "\n",
    "        if token_count == 0:\n",
    "            print(f\"[Attn] Skipping batch {batch_idx} (no valid targets)\")\n",
    "            continue\n",
    "        loss = loss_accum / token_count\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"[Attn] Skipping batch {batch_idx} due to invalid loss value\")\n",
    "            continue\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            list(encoder_attn.parameters()) + list(decoder_attn.parameters()),\n",
    "            max_norm=1.0\n",
    "        )\n",
    "\n",
    "        optimizer_attn.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if len(train_loader) == 0:\n",
    "        raise RuntimeError(\"Training loader is empty after preprocessing\")\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Attn] Epoch {epoch}/{num_epochs_attn}, Training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    encoder_attn.eval(); decoder_attn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for reactant_batch, product_batch in val_loader:\n",
    "            reactant_batch = reactant_batch.to(device)\n",
    "            product_batch = product_batch.to(device)\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder_attn(reactant_batch)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = torch.full(\n",
    "                (reactant_batch.size(0), 1),\n",
    "                token_to_idx[SOS_TOKEN],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            for t in range(max_len_prod):\n",
    "                output_logits, decoder_hidden, attn_weights = decoder_attn(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                output_logits = output_logits.squeeze(1)\n",
    "                if not torch.isfinite(output_logits).all():\n",
    "                    continue\n",
    "                target_token = product_batch[:, t]\n",
    "                mask = target_token != token_to_idx[PAD_TOKEN]\n",
    "                if mask.any():\n",
    "                    per_token = criterion_attn(output_logits, target_token)\n",
    "                    val_loss += (per_token * mask).sum().item()\n",
    "                    val_tokens += mask.sum().item()\n",
    "\n",
    "                # Teacher forcing during validation\n",
    "                decoder_input = target_token.unsqueeze(1)\n",
    "\n",
    "    if len(val_loader) == 0 or val_tokens == 0:\n",
    "        raise RuntimeError(\"Validation loader produced no valid targets\")\n",
    "    val_loss = val_loss / val_tokens\n",
    "    print(f\"[Attn] Validation loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PCQz9NtWnKE"
   },
   "outputs": [],
   "source": [
    "def predict_beam_search(reactant_seq, beam_width=5, max_len=100):\n",
    "    # Encode the input\n",
    "    encoder_outputs, encoder_hidden = encoder_attn(reactant_seq)\n",
    "\n",
    "    # Initialize the beam with the start token\n",
    "    # Each beam is (sequence_so_far, hidden_state, cumulative_log_prob)\n",
    "    beams = [([token_to_idx[SOS_TOKEN]], encoder_hidden, 0.0)]\n",
    "    completed_sequences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Expand beams up to max_len\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "\n",
    "            for seq, hidden, log_prob in beams:\n",
    "                # If this sequence already ended with EOS, carry it over to completed\n",
    "                if seq[-1] == token_to_idx[EOS_TOKEN]:\n",
    "                    completed_sequences.append((seq, log_prob))\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, get the next token probabilities\n",
    "                last_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                output_logits, new_hidden, attn_weights = decoder_attn(\n",
    "                    last_token, hidden, encoder_outputs\n",
    "                )\n",
    "                output_logits = output_logits.squeeze(1)  # (1, vocab_size)\n",
    "\n",
    "                # Convert to log probabilities\n",
    "                probs = torch.log_softmax(output_logits, dim=1)  # (1, vocab_size)\n",
    "                probs = probs.cpu().numpy().flatten()\n",
    "\n",
    "                # Get top beam_width next tokens\n",
    "                top_indices = np.argsort(probs)[-beam_width:][::-1]\n",
    "\n",
    "                for idx in top_indices:\n",
    "                    new_seq = seq + [int(idx)]\n",
    "                    new_log_prob = log_prob + float(probs[idx])\n",
    "\n",
    "                    # Pass along detached hidden state\n",
    "                    new_hidden_detached = (\n",
    "                        new_hidden[0].detach().clone(),\n",
    "                        new_hidden[1].detach().clone()\n",
    "                    )\n",
    "\n",
    "                    new_beams.append((new_seq, new_hidden_detached, new_log_prob))\n",
    "\n",
    "            # If we didn\u2019t expand any (all beams ended), break\n",
    "            if len(new_beams) == 0:\n",
    "                break\n",
    "\n",
    "            # Keep only the best beams (highest log_prob first)\n",
    "            new_beams.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "\n",
    "            # Stop if all beams have produced EOS\n",
    "            all_ended = all(seq[-1] == token_to_idx[EOS_TOKEN] for seq, _, _ in beams)\n",
    "            if all_ended:\n",
    "                completed_sequences.extend([(seq, log_prob) for seq, _, log_prob in beams])\n",
    "                break\n",
    "        else:\n",
    "            # If loop finishes without break (max_len reached), add remaining beams\n",
    "            completed_sequences.extend([(seq, log_prob) for seq, _, log_prob in beams])\n",
    "\n",
    "    # Sort completed sequences by log probability\n",
    "    completed_sequences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the token sequences (excluding SOS and trimming after EOS)\n",
    "    top_sequences = []\n",
    "    for seq, log_prob in completed_sequences[:beam_width]:\n",
    "        # Remove initial SOS\n",
    "        if seq and seq[0] == token_to_idx[SOS_TOKEN]:\n",
    "            seq = seq[1:]\n",
    "        # Trim after EOS if present\n",
    "        if token_to_idx[EOS_TOKEN] in seq:\n",
    "            seq = seq[:seq.index(token_to_idx[EOS_TOKEN])]\n",
    "        top_sequences.append(seq)\n",
    "\n",
    "    return top_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwm62avQWvSx",
    "outputId": "0913557e-3c42-4cd5-dcb6-ab5e0391b65c"
   },
   "outputs": [],
   "source": [
    "encoder_attn.eval()\n",
    "decoder_attn.eval()\n",
    "\n",
    "top1_correct = 0\n",
    "top3_correct = 0\n",
    "top5_correct = 0\n",
    "valid_top1 = 0\n",
    "total = 0\n",
    "\n",
    "for reactant_seq, true_product_seq in test_loader:\n",
    "    reactant_seq = reactant_seq.to(device)\n",
    "    true_product_seq = true_product_seq.to(device)\n",
    "\n",
    "    # Ensure single sample for beam search\n",
    "    reactant_seq = reactant_seq.unsqueeze(0) if reactant_seq.dim() == 1 else reactant_seq\n",
    "    if reactant_seq.size(0) != 1:\n",
    "        reactant_seq = reactant_seq[0].unsqueeze(0)\n",
    "\n",
    "    # Extract true product SMILES (remove PAD and EOS)\n",
    "    true_tokens = torch.flatten(true_product_seq).tolist()\n",
    "\n",
    "    if token_to_idx[PAD_TOKEN] in true_tokens:\n",
    "        true_tokens = true_tokens[:true_tokens.index(token_to_idx[PAD_TOKEN])]\n",
    "    if true_tokens and true_tokens[-1] == token_to_idx[EOS_TOKEN]:\n",
    "        true_tokens = true_tokens[:-1]\n",
    "\n",
    "    true_smiles = \"\".join([idx_to_token[t] for t in true_tokens])\n",
    "    true_canon = canonicalize_smiles(true_smiles)\n",
    "\n",
    "    # Beam search predictions\n",
    "    top_sequences = predict_beam_search(\n",
    "        reactant_seq, beam_width=5, max_len=max_len_prod\n",
    "    )\n",
    "\n",
    "    # Convert token sequences into canonical SMILES\n",
    "    pred_smiles_list = []\n",
    "    for seq in top_sequences:\n",
    "        smiles = \"\".join([idx_to_token[t] for t in seq])\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            can = Chem.MolToSmiles(mol, canonical=True)\n",
    "        else:\n",
    "            can = None  # invalid SMILES\n",
    "        pred_smiles_list.append(can)\n",
    "\n",
    "    # Count valid top-1 predictions\n",
    "    if pred_smiles_list:\n",
    "        if pred_smiles_list[0] is not None:\n",
    "            valid_top1 += 1\n",
    "\n",
    "    # Check for correct matches in top-k\n",
    "    if true_canon is not None:\n",
    "        for rank, pred in enumerate(pred_smiles_list):\n",
    "            if pred is not None and pred == true_canon:\n",
    "                if rank == 0:\n",
    "                    top1_correct += 1\n",
    "                if rank < 3:\n",
    "                    top3_correct += 1\n",
    "                if rank < 5:\n",
    "                    top5_correct += 1\n",
    "                break  # stop after first correct match\n",
    "\n",
    "    total += 1\n",
    "\n",
    "# Final metrics\n",
    "top1_acc = top1_correct / total * 100\n",
    "top3_acc = top3_correct / total * 100\n",
    "top5_acc = top5_correct / total * 100\n",
    "valid_percent = valid_top1 / total * 100\n",
    "\n",
    "print(f\"Attention Seq2Seq Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "print(f\"Attention Seq2Seq Top-3 Accuracy: {top3_acc:.2f}%\")\n",
    "print(f\"Attention Seq2Seq Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "print(f\"Valid Top-1 Predictions: {valid_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wuNdLN7W3yT",
    "outputId": "ee3a7d26-719a-44eb-dc49-08d1f9d88fb1"
   },
   "outputs": [],
   "source": [
    "# Compare an example with and without attention\n",
    "for i in range(3):\n",
    "    reactant = test_enc_reactants[i].unsqueeze(0).to(device)\n",
    "    true_prod = test_products[i]\n",
    "\n",
    "    # Baseline greedy\n",
    "    base_tokens = predict_greedy(reactant)\n",
    "    base_smiles = \"\".join([idx_to_token[t] for t in base_tokens])\n",
    "\n",
    "    # Attention greedy (fair comparison)\n",
    "    encoder_attn.eval()\n",
    "    decoder_attn.eval()\n",
    "    attn_tokens = []\n",
    "    with torch.no_grad():\n",
    "        enc_out, enc_h = encoder_attn(reactant)\n",
    "        dec_h = enc_h\n",
    "        dec_input = torch.full(\n",
    "            (1, 1),\n",
    "            token_to_idx[SOS_TOKEN],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        for t in range(max_len_prod):\n",
    "            out_logits, dec_h, attn_w = decoder_attn(dec_input, dec_h, enc_out)\n",
    "            top_token = out_logits.squeeze(1).argmax(dim=1)\n",
    "            if top_token.item() == token_to_idx[EOS_TOKEN]:\n",
    "                break\n",
    "            attn_tokens.append(top_token.item())\n",
    "            dec_input = top_token.unsqueeze(1)\n",
    "\n",
    "    attn_smiles = \"\".join([idx_to_token[t] for t in attn_tokens])\n",
    "\n",
    "    # Print comparison\n",
    "    print(f\"Reactants: {test_reactants[i]}\")\n",
    "    print(f\"True Product: {true_prod}\")\n",
    "    print(f\"Baseline Pred (greedy): {base_smiles}\")\n",
    "    print(f\"Attention Pred (greedy): {attn_smiles}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "817y-sLdux07",
    "outputId": "0a96523f-8fc8-4d7f-a874-e89c434bab0a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example reactant and product SMILES (with simple tokenization by character for demo)\n",
    "reactant_smiles = \"CCBr.O\"  # e.g., bromoethane and water (tokens: C, C, Br, ., O)\n",
    "product_smiles = \"CCO\"      # e.g., ethanol (tokens: C, C, O)\n",
    "\n",
    "# Suppose the model's attention weights matrix has shape [len(product_tokens) x len(reactant_tokens)]\n",
    "# We create a dummy attention matrix for illustration:\n",
    "input_tokens = list(reactant_smiles)   # ['C','C','B','r','.','O']\n",
    "output_tokens = list(product_smiles)   # ['C','C','O']\n",
    "attention_weights = np.array([\n",
    "    [0.8, 0.2, 0.0, 0.0, 0.0, 0.0],  # attention for output token 1 ('C')\n",
    "    [0.1, 0.7, 0.2, 0.0, 0.0, 0.0],  # attention for output token 2 ('C')\n",
    "    [0.0, 0.0, 0.1, 0.0, 0.0, 0.9]   # attention for output token 3 ('O')\n",
    "])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(attention_weights, annot=True, cmap=\"YlGnBu\",\n",
    "            xticklabels=input_tokens, yticklabels=output_tokens)\n",
    "plt.xlabel(\"Reactant SMILES Tokens\")\n",
    "plt.ylabel(\"Product SMILES Tokens\")\n",
    "plt.title(\"Attention Weight Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzzqtsPVuzSJ",
    "outputId": "c08b74b0-89d9-459a-8fe2-3bd87d620ed6"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# True vs predicted product for a sample reaction (hypothetical failure case)\n",
    "true_smiles = \"CCO\"      # e.g., true product is ethanol\n",
    "pred_smiles = \"CCCl\"     # e.g., model predicted chloroethane (wrong product)\n",
    "\n",
    "# Parse SMILES to molecules\n",
    "true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "\n",
    "# 1. Check if predicted SMILES was chemically parsable\n",
    "if pred_mol is None:\n",
    "    print(\"Predicted SMILES is invalid and could not be parsed by RDKit.\")\n",
    "else:\n",
    "    print(\"Predicted SMILES parsed successfully (chemically valid molecule).\")\n",
    "\n",
    "# 2. Compare molecular formulas for atom conservation\n",
    "true_formula = rdMolDescriptors.CalcMolFormula(true_mol)\n",
    "pred_formula = rdMolDescriptors.CalcMolFormula(pred_mol)\n",
    "print(f\"True product formula: {true_formula}\")\n",
    "print(f\"Predicted product formula: {pred_formula}\")\n",
    "if true_formula != pred_formula:\n",
    "    print(\"Atom count mismatch \u2013 the model's prediction has a different formula (atoms lost or gained).\")\n",
    "else:\n",
    "    print(\"Formulas match \u2013 predicted product has the same atoms as the true product (but possibly arranged differently).\")\n",
    "\n",
    "# 3. (If needed) check for specific atom mismatches:\n",
    "true_atoms = sorted([atom.GetSymbol() for atom in true_mol.GetAtoms()])\n",
    "pred_atoms = sorted([atom.GetSymbol() for atom in pred_mol.GetAtoms()])\n",
    "print(f\"True atoms: {true_atoms}\")\n",
    "print(f\"Predicted atoms: {pred_atoms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MD81Ozwqu03u",
    "outputId": "d401add5-c5ad-4ed9-f371-6d64fab75ae3"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors, AllChem, DataStructs\n",
    "\n",
    "# Example SMILES lists\n",
    "true_products = [\"CCCO\", \"CCO\", \"invalid_smiles\"]\n",
    "predicted_products = [\"CCCO\", \"CC\", \"C(C)(C)O\"]\n",
    "\n",
    "sims = []\n",
    "\n",
    "for true_smiles, pred_smiles in zip(true_products, predicted_products):\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "    pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "\n",
    "    if true_mol is None or pred_mol is None:\n",
    "        sims.append(0.0)\n",
    "        continue\n",
    "\n",
    "    fp_true = rdMolDescriptors.GetMorganFingerprintAsBitVect(true_mol, radius=2, nBits=1024)\n",
    "    fp_pred = rdMolDescriptors.GetMorganFingerprintAsBitVect(pred_mol, radius=2, nBits=1024)\n",
    "\n",
    "    sim = DataStructs.TanimotoSimilarity(fp_true, fp_pred)\n",
    "    sims.append(sim)\n",
    "\n",
    "# Summary stats\n",
    "average_sim = sum(sims) / len(sims)\n",
    "print(f\"Average Tanimoto similarity on test set = {average_sim:.3f}\")\n",
    "print(\"Sample similarities:\", sims[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiO7zkSju164",
    "outputId": "9f64a3b4-a965-47d3-c676-3ba0714f2f75"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors, DataStructs\n",
    "\n",
    "# Example SMILES lists with some invalid entries\n",
    "true_products = [\"CCCO\", \"CCO\", \"...\"]\n",
    "predicted_products = [\"CCCO\", \"CC\", \"...\"]\n",
    "\n",
    "sims = []\n",
    "\n",
    "for true_smiles, pred_smiles in zip(true_products, predicted_products):\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "    pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "\n",
    "    # Skip or assign similarity 0.0 if parsing fails\n",
    "    if true_mol is None or pred_mol is None:\n",
    "        sims.append(0.0)\n",
    "        continue\n",
    "\n",
    "    fp_true = rdMolDescriptors.GetMorganFingerprintAsBitVect(true_mol, radius=2, nBits=1024)\n",
    "    fp_pred = rdMolDescriptors.GetMorganFingerprintAsBitVect(pred_mol, radius=2, nBits=1024)\n",
    "\n",
    "    sim = DataStructs.TanimotoSimilarity(fp_true, fp_pred)\n",
    "    sims.append(sim)\n",
    "\n",
    "# Summary statistics\n",
    "average_sim = sum(sims) / len(sims)\n",
    "print(f\"Average Tanimoto similarity on test set = {average_sim:.3f}\")\n",
    "print(\"Sample similarities:\", sims[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "4hTL6u04u51K",
    "outputId": "bd5f61a8-b6df-4af7-946f-165b12c1c446"
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "\n",
    "# Define true and predicted product SMILES for two example reactions\n",
    "true_smiles1 = \"CCCO\"    # 1-propanol (true product in example 1)\n",
    "pred_smiles1 = \"CCCO\"    # model prediction (same, correct)\n",
    "true_smiles2 = \"CCO\"     # ethanol (true product in example 2)\n",
    "pred_smiles2 = \"CC\"      # ethane (model's incorrect prediction for example 2)\n",
    "\n",
    "# Convert to RDKit Mol objects\n",
    "mol_true1 = Chem.MolFromSmiles(true_smiles1)\n",
    "mol_pred1 = Chem.MolFromSmiles(pred_smiles1)\n",
    "mol_true2 = Chem.MolFromSmiles(true_smiles2)\n",
    "mol_pred2 = Chem.MolFromSmiles(pred_smiles2)\n",
    "\n",
    "# Draw molecules in a 2x2 grid: columns = (True, Predicted), rows = (Example1, Example2)\n",
    "mols = [mol_true1, mol_pred1, mol_true2, mol_pred2]\n",
    "legends = [\n",
    "    \"True Product (Example 1)\", \"Predicted Product (Example 1)\",\n",
    "    \"True Product (Example 2)\", \"Predicted Product (Example 2)\"\n",
    "]\n",
    "img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(300,200), legends=legends)\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "Ub9FF1kFu7-p",
    "outputId": "ede21151-848f-4108-de9f-d4540058884a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_classes = []  # fill with class labels if available\n",
    "if train_classes:\n",
    "    classes, counts = np.unique(train_classes, return_counts=True)\n",
    "    total = len(train_classes)\n",
    "    class_weight = {cls: total/count for cls, count in zip(classes, counts)}\n",
    "    print(\"Class weights:\", class_weight)\n",
    "else:\n",
    "    print(\"No class labels provided; skipping class-weight demo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "_m7izo6qvnUX",
    "outputId": "00b9c20d-617a-4c26-af22-52096063f74f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs, Chem\n",
    "\n",
    "true_smiles_list = []\n",
    "pred_smiles_list = []\n",
    "class_list = []\n",
    "\n",
    "if not true_smiles_list:\n",
    "    print(\"Add true/pred SMILES and class labels to compute per-class metrics.\")\n",
    "else:\n",
    "    df = pd.DataFrame({\n",
    "        \"reaction_class\": class_list,\n",
    "        \"true_smiles\": true_smiles_list,\n",
    "        \"pred_smiles\": pred_smiles_list\n",
    "    })\n",
    "    df[\"correct\"] = df[\"true_smiles\"] == df[\"pred_smiles\"]\n",
    "    acc_per_class = df.groupby(\"reaction_class\")[\"correct\"].mean()\n",
    "    print(\"Top-1 Accuracy by class:\")\n",
    "    print(acc_per_class)\n",
    "\n",
    "    def tanimoto_smiles(smi1, smi2):\n",
    "        mol1 = Chem.MolFromSmiles(smi1); mol2 = Chem.MolFromSmiles(smi2)\n",
    "        if mol1 is None or mol2 is None:\n",
    "            return 0.0\n",
    "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=1024)\n",
    "        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=1024)\n",
    "        return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "    df[\"tanimoto_sim\"] = df.apply(lambda row: tanimoto_smiles(row[\"true_smiles\"], row[\"pred_smiles\"]), axis=1)\n",
    "    sim_per_class = df.groupby(\"reaction_class\")[\"tanimoto_sim\"].mean()\n",
    "    print(\"\\nAverage Tanimoto similarity by class:\")\n",
    "    print(sim_per_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF_cjZMrvoSF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rxn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}